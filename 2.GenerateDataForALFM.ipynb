{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data for Amazon Lookout for Metrics \n",
    "\n",
    "Amazon Lookout for Metrics(ALFM) needs at a minimum a new stream of information that reflects your dataset defined in the previous notebook, it can also leverage historical data to train on beforehand. In this notebook we will walk through the process of curating a historical dataset as well as a forward looking on. Once ALFM identifies the historical data it will start to train better models behind the scenes, then the forward looking data will be crawled in human scale time going forward and used as an inference input. If an anomaly is found, it will be sent to the SNS queue defined previously.\n",
    "\n",
    "First restore the variables from the previous notebook and then import the libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import botocore.config\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in the last notebook, connect to AWS through the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name = \"us-east-1\")\n",
    "poirot = session.client(\"poirot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the logic for generating synthetic data, it will be the basis for how we build the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(start_time, end_time, normal=True, closed=None):\n",
    "    if not normal:\n",
    "        print(\"=> generate_synthetic_data({}, {}, {}, {})\".format(start_time, end_time, normal, closed))\n",
    "    result = []\n",
    "\n",
    "    date_range = pd.date_range(start=start_time, end=end_time, freq='1S', closed=closed)\n",
    "    for ts in date_range:\n",
    "        str_ts = ts.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        product = 'A' # dimension\n",
    "\n",
    "        count = np.random.randint(10)\n",
    "        if normal:\n",
    "            mu, sigma = 100, 10 # mean and standard deviation\n",
    "        else:\n",
    "            mu, sigma = 70, 30 # mean and standard deviation\n",
    "        price = round(np.random.normal(mu, sigma), 2)\n",
    "        result.append([str_ts, product, count, price])\n",
    "        #print(\"{}, {}, {}, {}\".format(ts, product, count, price))\n",
    "        \n",
    "        product = 'B'  # dimension\n",
    "        count = np.random.randint(10, 20)\n",
    "        if normal:\n",
    "            mu, sigma = 50, 5 # mean and standard deviation\n",
    "        else:\n",
    "            mu, sigma = 50, 1 # mean and standard deviation\n",
    "        price = round(np.random.normal(mu, sigma), 2)\n",
    "        result.append([str_ts, product, count, price])\n",
    "        #print(\"{}, {}, {}, {}\".format(ts, product, count, price))\n",
    "    \n",
    "    #print(\"<= generate_synthetic_data(): {} rows\".format(len(result)))\n",
    "    return result\n",
    "\n",
    "\n",
    "def write_data_to_csv(data, csv_path):\n",
    "    with open(csv_path, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        writer.writerow([\"timestamp\", \"product\", \"count\", \"price\"])\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "\n",
    "def write_csv_to_s3(csv_path, s3_bucket, s3_key):\n",
    "    s3 = boto3.client('s3', config=botocore.config.Config(s3={'addressing_style':'path'}))\n",
    "    with open(csv_path, \"rb\") as f:\n",
    "        s3.upload_fileobj(f, s3_bucket, s3_key)\n",
    "    return \"s3://%s/%s\" % ( s3_bucket, s3_key )\n",
    "\n",
    "\n",
    "def simulate_data(timestamp, normal=True):    \n",
    "\n",
    "    time = dateutil.parser.parse(str(timestamp))\n",
    "    #print(\"using time = {}\".format(time))\n",
    "        \n",
    "    interval_minutes = int(5)\n",
    "    \n",
    "    interval = datetime.timedelta(minutes = interval_minutes) # generate new data every time interval, e.g. 5 minutes\n",
    "    one_second = datetime.timedelta(seconds = 1)\n",
    "\n",
    "    start_time = time - interval\n",
    "    data = generate_synthetic_data(start_time, time, normal, closed='left')\n",
    "\n",
    "    csv_file = \"{}-{:02d}-{:02d}-{:02d}-{:02d}.csv\".format(start_time.year, start_time.month, start_time.day, start_time.hour, start_time.minute)\n",
    "    csv_path = \"/tmp/{}\".format(csv_file)\n",
    "    write_data_to_csv(data, csv_path)\n",
    "    #print(\"wrote to {}\".format(csv_file))\n",
    "    \n",
    "   # print(\"project = {}, s3_bucket = {}\". format(project, s3_bucket))\n",
    "    if project is None or s3_bucket is None:\n",
    "        print(\"both 'project' and 's3_bucket' must be set as environment variables to upload to S3, so skipping S3 upload\")\n",
    "    else:\n",
    "        s3_key = \"{}/{:02d}/{:02d}/{:02d}-{:02d}/{}\".format(start_time.year, start_time.month, start_time.day, start_time.hour, start_time.minute, csv_file)\n",
    "        s3_url = write_csv_to_s3(csv_path, s3_bucket, s3_key)\n",
    "        #print(\"uploaded {}\".format(s3_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running The Simulation\n",
    "\n",
    "Knowing that the data needs to exist in 5 minute intervals, the next question is when are we going to start. To make that easier we will simply start with the current date and then generating 1000 data points. Depending on when you start this exercise, that will provide a sample of historical data as well as some forward looking information. \n",
    "\n",
    "Once we have the range we will then be able to invoke a function that for each slice in the range will call our function to create synthetic data, then write it to s3. From there you can sit back and wait to be notified of any anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04\n"
     ]
    }
   ],
   "source": [
    "# Get Current Date as a string:\n",
    "todays_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(todays_date)\n",
    "#todays_date = \"2020-10-22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a time series of 1000 5 min slices from the start of the day\n",
    "times = pd.date_range(todays_date, periods=1000, freq='5Min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will actually generate the data and upload it to S3 for you. The output below is specifically for the anomalous events. Save that in a textfile to validate the alerts were delivered correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> generate_synthetic_data(2020-11-04 00:20:00, 2020-11-04 00:25:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 02:25:00, 2020-11-04 02:30:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 02:45:00, 2020-11-04 02:50:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 04:10:00, 2020-11-04 04:15:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 07:30:00, 2020-11-04 07:35:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 09:00:00, 2020-11-04 09:05:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 09:45:00, 2020-11-04 09:50:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 09:55:00, 2020-11-04 10:00:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 10:20:00, 2020-11-04 10:25:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 14:20:00, 2020-11-04 14:25:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 15:05:00, 2020-11-04 15:10:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 15:40:00, 2020-11-04 15:45:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 15:50:00, 2020-11-04 15:55:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 16:25:00, 2020-11-04 16:30:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 16:50:00, 2020-11-04 16:55:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 17:25:00, 2020-11-04 17:30:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 18:20:00, 2020-11-04 18:25:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 20:55:00, 2020-11-04 21:00:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 22:30:00, 2020-11-04 22:35:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 22:45:00, 2020-11-04 22:50:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-04 23:45:00, 2020-11-04 23:50:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 04:35:00, 2020-11-05 04:40:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 04:50:00, 2020-11-05 04:55:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 05:35:00, 2020-11-05 05:40:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 09:20:00, 2020-11-05 09:25:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 10:35:00, 2020-11-05 10:40:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 11:25:00, 2020-11-05 11:30:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 11:55:00, 2020-11-05 12:00:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 13:15:00, 2020-11-05 13:20:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 15:35:00, 2020-11-05 15:40:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 15:50:00, 2020-11-05 15:55:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 18:30:00, 2020-11-05 18:35:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 23:00:00, 2020-11-05 23:05:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-05 23:45:00, 2020-11-05 23:50:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-06 10:45:00, 2020-11-06 10:50:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-06 13:20:00, 2020-11-06 13:25:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-06 14:50:00, 2020-11-06 14:55:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-06 17:30:00, 2020-11-06 17:35:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-06 17:55:00, 2020-11-06 18:00:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-06 19:30:00, 2020-11-06 19:35:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-06 20:30:00, 2020-11-06 20:35:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-06 20:40:00, 2020-11-06 20:45:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-06 20:45:00, 2020-11-06 20:50:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-06 21:00:00, 2020-11-06 21:05:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-06 23:20:00, 2020-11-06 23:25:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-07 00:15:00, 2020-11-07 00:20:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-07 01:00:00, 2020-11-07 01:05:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-07 03:20:00, 2020-11-07 03:25:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-07 04:35:00, 2020-11-07 04:40:00, False, left)\n",
      "=> generate_synthetic_data(2020-11-07 08:00:00, 2020-11-07 08:05:00, False, left)\n"
     ]
    }
   ],
   "source": [
    "for ts in times:\n",
    "    chance = random.randint(1,100)\n",
    "    if chance <= 5:\n",
    "        simulate_data(timestamp=ts, normal=False)\n",
    "    else:\n",
    "        simulate_data(timestamp=ts, normal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Next?\n",
    "\n",
    "Note the timestamps above, those are the anomalous events that have been written into your dataset, keep a lookout for any alerts that hit your mobile number and you can see how the service is performing with this synthetic dataset.\n",
    "\n",
    "You can also view the anomalies in the console later as well for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
