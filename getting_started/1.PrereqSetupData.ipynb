{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite: Set-up a S3 bucket\n",
    "\n",
    "This notebook assumes you completed the earlier steps in `README.md`, if you did not, go back and do that, the notebook will wait patiently for you to come back.\n",
    "\n",
    "In this notebook you will leverage a provided e-commerce dataset to demonstrate the functionality of Amazon Lookout For Metrics(L4M). This is meant to be educational and to guide you through an approach that should work well for your own datasets. \n",
    "\n",
    "If you are looking to leverage your own dataset, simply export it as a CSV and follow along here after the extraction steps. \n",
    "\n",
    "## Data set-up workflow:\n",
    "\n",
    "1. Create a bucket\n",
    "2. Uncompress the dataset\n",
    "3. Format the data to be read by Lookout For Metrics (Already Done)\n",
    "3. Save data to bucket\n",
    "\n",
    "After these steps have been completed you are ready to get started exploring the data with Amazon Lookout For Metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import utility\n",
    "import synth_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a bucket\n",
    "\n",
    "As mentioned above, data needs to exist somewhere. Run the next cell to create a bucket for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "s3_bucket = account_id + \"-lookoutmetrics-lab4\"\n",
    "\n",
    "region = \"us-west-2\"\n",
    "utility.create_bucket(s3_bucket, region=region)\n",
    "\n",
    "s3_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncompress dataset\n",
    "\n",
    "Next uncompress the archive that was provided, you would skip this if bringing your own data, however you should still create a folder named `data`. You can do that by right clicking in the panel to the left and creating a new folder with the diaglog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dirname = os.path.join(\"./data\")\n",
    "\n",
    "if os.path.exists(data_dirname):\n",
    "    shutil.rmtree(data_dirname)\n",
    "os.makedirs(data_dirname)\n",
    "\n",
    "zip_filename = os.path.join(\"./ecommerce.zip\")\n",
    "\n",
    "with zipfile.ZipFile( zip_filename, \"r\" ) as zip_fd:\n",
    "    zip_fd.extractall(data_dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you proceed to the next step let's take a quick look at the folder structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths = utility.DisplayablePath.make_tree(pathlib.Path('data'))\n",
    "for path in paths:\n",
    "    print(path.displayable())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Specifically, notice that we only have one `input.csv` in the `backtest`. Whereas, data in the `live` folder is broken down into days (ex: `20210101` for January 1, 2021) and hours (ex: `0200` for 2:00AM). \n",
    "\n",
    "This path structure for the live data is *CRITICAL* for using the service to detect live information, your own datasets must be built in a similar manner so that Lookout for Metrics will understand how to find data in the future. Soon we will have a sample that showcases how to stream data from Lambda or Kinesis into a structure like this.\n",
    "\n",
    "**Note: it is totally fine to create these live data points later.** The point is to upload data just before the time for anomaly detection, so you will need some form of automated process to do this.\n",
    "\n",
    "Also, notice that our data goes far into the future. Of course, this is unrealistic of any real-world scenarios but it works for this demontration.\n",
    "\n",
    "Now when you take a quick look into the data, you will notice the schema for `backtest` and `live` data is identical. \n",
    "\n",
    "If you are providing your own data to understand the service, again it is totally fine to just use your backtesting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_df = pd.read_csv('data/ecommerce/backtest/input.csv')\n",
    "backtest_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_sample_df = pd.read_csv('data/ecommerce/live/20210308/0000/20210308_000000.csv')\n",
    "live_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we have live data into the future, run the cell below, it will update your data folders to have an up to date history as well as data well into the future. This is crucial to make sure that you generate alerts in the future when using continuous mode. If you aren't going that route, feel free to skip this step and move onto the bucket syncing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_data.generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to bucket\n",
    "\n",
    "Finally, let save the data into to our s3 bucket. Note the `--quiet` at the end of the command, this will prevent the output from consuming a ton of resources in this browser window(you'd see thousands of files listed here without it). It will take a few minutes to complete.\n",
    "\n",
    "**Important:** In the cell below there is a folder called `ecommerce` update it to whatever the name of your dataset folder is. If you just placed your content inside the `data` folder, delete the `ecommerce` bit and leave one trailing slash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 sync {data_dirname}/ecommerce/ s3://{s3_bucket}/ecommerce/ --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier on yourself we are going to leverage the magic functions of Ipython in order to save a few variables for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store s3_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring IAM\n",
    "\n",
    "Before Lookout For Metrics can read your data an IAM role will need to be created so that the service can communicate with S3. Additionally you will need to enable SNS support if you wish to recieve alerts later. The cell below will create that role for you and then return its ARN so that you can use it later via the notebooks or the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = \"L4MTestRole\"\n",
    "role_arn = utility.get_or_create_iam_role(role_name)\n",
    "%store role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "With data loaded into S3 you can now move on to working with Lookout for Metrics. It is recommended that you explore your historical data via Backtesting first, so continue on to `2.BacktestingWithHistoricalData.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
