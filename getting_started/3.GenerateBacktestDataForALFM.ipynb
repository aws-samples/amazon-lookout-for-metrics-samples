{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Backtesting Data for Amazon Lookout for Metrics\n",
    "\n",
    "Amazon Lookout for Metrics(ALFM) also supports backtesting against your historical information. In this notebook you will generate sample data, upload it to s3 and then guided through the backtesting functionality within the console. Once the backtesting job has completed you can see all of the anomalies that ALFM detected in the last 20% of your historical data. From here you can begin to unpack the kinds of results you will see from ALFM in the future when you start streaming in new data. **NOTE YOU MUST CREATE A NEW DETECTOR TO LEVERAGE REAL TIME DATA. BACKTESTING IS ONLY FOR EXPLORATION.**\n",
    "\n",
    "This notebook does assume that you already executed the real time example in the previous notebooks, if you have not, go back and complete those first, then give backtesting a whirl while you are waiting for the real time anomalies to alert you!\n",
    "\n",
    "First restore the variables from the previous notebook and then import the libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import botocore.config\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in the last notebook, connect to AWS through the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name = \"us-east-1\")\n",
    "poirot = session.client(\"poirot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the logic for generating synthetic data, it will be the basis for how we build the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(logfile, start_time, end_time, normal=True, closed=None):\n",
    "    if not normal:\n",
    "        logfile.writelines(\"=> generate_synthetic_data({}, {}, {}, {})\".format(start_time, end_time, normal, closed))\n",
    "        logfile.writelines(\"\\n\")\n",
    "    result = []\n",
    "\n",
    "    date_range = pd.date_range(start=start_time, end=end_time, freq='5Min', closed=closed)\n",
    "    for ts in date_range:\n",
    "        str_ts = ts.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        product = 'A' # dimension\n",
    "\n",
    "        count = np.random.randint(10)\n",
    "        if normal:\n",
    "            mu, sigma = 100, 10 # mean and standard deviation\n",
    "        else:\n",
    "            mu, sigma = 70, 30 # mean and standard deviation\n",
    "        price = round(np.random.normal(mu, sigma), 2)\n",
    "        result.append([str_ts, product, count, price])\n",
    "        #print(\"{}, {}, {}, {}\".format(ts, product, count, price))\n",
    "        \n",
    "        product = 'B'  # dimension\n",
    "        count = np.random.randint(10, 20)\n",
    "        if normal:\n",
    "            mu, sigma = 50, 5 # mean and standard deviation\n",
    "        else:\n",
    "            mu, sigma = 50, 1 # mean and standard deviation\n",
    "        price = round(np.random.normal(mu, sigma), 2)\n",
    "        result.append([str_ts, product, count, price])\n",
    "        #print(\"{}, {}, {}, {}\".format(ts, product, count, price))\n",
    "    \n",
    "    #print(\"<= generate_synthetic_data(): {} rows\".format(len(result)))\n",
    "    return result\n",
    "\n",
    "\n",
    "def simulate_data(logfile, csvfile, timestamp, normal=True):    \n",
    "\n",
    "    time = dateutil.parser.parse(str(timestamp))\n",
    "    #print(\"using time = {}\".format(time))\n",
    "        \n",
    "    interval_minutes = int(5)\n",
    "    \n",
    "    interval = datetime.timedelta(minutes = interval_minutes) # generate new data every time interval, e.g. 5 minutes\n",
    "    one_second = datetime.timedelta(seconds = 1)\n",
    "\n",
    "    start_time = time - interval\n",
    "    data = generate_synthetic_data(logfile, start_time, time, normal, closed='left')\n",
    "    \n",
    "    for row in data:\n",
    "        csvfile.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Simulated Data\n",
    "\n",
    "At present ALFM supports 5,000 historical data points for backtesting, in this case we are going to generate anomalies for 5,000 intervals of 5 minutes historical. This backtesting will also only consider points starting from the moment when you create your detector. For that reason we will start with today's date then walk, backwards 17 days(roughly 5,000 5 min slots). From here you will have a perfect dataset for validating anomalies in the past.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = datetime.datetime.now()\n",
    "# Delta of 17 days is roughly 5,000 data points for the service\n",
    "delta_date = datetime.timedelta(days=17)\n",
    "start_date = todays_date - delta_date\n",
    "start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "print(start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a time series  5 min slices from the start date\n",
    "times = pd.date_range(start_date, periods=5000, freq='5Min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below will actually generate the data and upload it to S3 for you. It will also create a file `anomalies.log` that contains all the anomalous events that were created, here we are using a 1% chance of an anomaly occuring in any period. You can update the probablity rate to be higher than 1 if you like, but if they become too frequent... they cease to be anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be an integer between 0 and 100\n",
    "probabilty_of_anomaly_in_percent = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First open the CSV file\n",
    "with open(\"anomalies.log\", 'w') as logfile:\n",
    "    with open(\"backtestdata.csv\", 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        writer.writerow([\"timestamp\", \"product\", \"count\", \"price\"])\n",
    "        for ts in times:\n",
    "            chance = random.randint(1,100)\n",
    "            if chance <= probabilty_of_anomaly_in_percent:\n",
    "                simulate_data(logfile, writer, timestamp=ts, normal=False)\n",
    "            else:\n",
    "                simulate_data(logfile, writer, timestamp=ts, normal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the Data\n",
    "s3 = boto3.client('s3', config=botocore.config.Config(s3={'addressing_style':'path'}))\n",
    "with open(\"backtestdata.csv\", \"rb\") as f:\n",
    "    s3.upload_fileobj(f, s3_bucket, \"backtestdata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Simulation\n",
    "\n",
    "Next you are ready to configure the backtesting job, you will need a few parameters for this which we can glean from these notebooks, but to begin, first open the ALFM console by opening a new browser window or tab and visiting: https://console.aws.amazon.com/poirot/home?region=us-east-1#landing. \n",
    "\n",
    "From here click the `Create detector` button. \n",
    "\n",
    "Next give it a simple name like `InitialBacktestDetector` note the name does need to be unique within your account. You can specify whatever you like for the description, then choose `Advertising` for the domain, and select an interval of `5 minute intervals`. If a custom encryption key was needed for this workload, this is where you would sepcify it. Note that ALFM will automatically use a custom generated key for you in the background if no key is provided. This ensures your information is always encrypted at rest and in transit with ALFM. Lastly here click `Create`.\n",
    "\n",
    "Now the dataset can be specified, click the `Add a dataset` button on the page.\n",
    "\n",
    "This is where we start to select the historical data that was just provided.\n",
    "\n",
    "Start by giving this a name such as `InitialBacktestDataSet`, GMT is perfectly fine for the timezone. For Data source, specify Amazon S3. Then select `Test`.\n",
    "\n",
    "The cell below will print the path to your data, copy and paste it in the historical data field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://059124553121poirottestbacktest/backtestdata.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"s3://\"+s3_bucket+\"/\"+\"backtestdata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defaults provided in the rest of the form are sufficent, click `Next`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to map the fields of your data into ALFM, to do that get started with the measures use the following configuration:\n",
    "\n",
    "```\n",
    "Fieldname: count\n",
    "Type: \"IMPRESSIONS\"\n",
    "Aggregate by: AVG\n",
    "```\n",
    "\n",
    "Then click `Add measure` and enter the next configuration:\n",
    "\n",
    "```\n",
    "Fieldname: price\n",
    "Type: \"REVENUE\"\n",
    "Aggregate by: SUM\n",
    "```\n",
    "\n",
    "Note here the aggregation is not used as we have specified data in the exact rate that it is expected to arrive. This means that there's no reason for these to do anything.\n",
    "\n",
    "Next click, `Add dimension`, and select `product`.\n",
    "\n",
    "Lastly the timestamp setup, select the dropdown of `timestamp` for the field, and enter: `yyyy-MM-dd HH:mm:ss` for your Format and click `Next`.\n",
    "\n",
    "To kick off the training and backtesting process click `Save and activate`. You will see a warning about costs, click `Activate`.\n",
    "\n",
    "The training process will run for a bit, go relax, take a walk, grab a coffee and come back in about 2 hours to continue on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
